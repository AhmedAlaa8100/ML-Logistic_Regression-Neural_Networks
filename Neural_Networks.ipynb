{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a91ad63",
   "metadata": {},
   "source": [
    "# Kaiming Normal Initialization (He Initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a21032",
   "metadata": {},
   "source": [
    "**The Core Problem: Bad Initialization**\n",
    "\n",
    "When we train a neural network, each layer transforms inputs with weights:\n",
    "\n",
    "ùë¶ = ùëäùë• + ùëè\n",
    "\n",
    "If we choose weights poorly:\n",
    "\n",
    "Activations can explode (grow too large)\n",
    "\n",
    "or vanish (shrink toward 0)\n",
    "\n",
    "or become identical (if all weights start equal, breaking learning symmetry)\n",
    "\n",
    "This makes training slow or impossible.\n",
    "\n",
    "So, we need to initialize weights carefully to keep signals flowing properly during forward and backward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be646f2",
   "metadata": {},
   "source": [
    "Developed by **Kaiming He et al. (2015)** for **ReLU networks**, this initialization ensures that:\n",
    "\n",
    "- The **output variance** of each layer roughly matches the **input variance**\n",
    "- This keeps activations ‚Äúalive‚Äù (not too big, not too small)\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$\n",
    "W_{ij} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- n: number of input units (inputs to a neuron)\n",
    "- The factor **2** compensates for the fact that **ReLU** sets about half of the activations to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b5443",
   "metadata": {},
   "source": [
    "| Initialization          | Effect                                        |\n",
    "| ----------------------- | --------------------------------------------- |\n",
    "| **Zeros**               | All neurons behave identically ‚Üí no learning  |\n",
    "| **Random small values** | Might cause vanishing/exploding gradients     |\n",
    "| **Kaiming Normal**      | Keeps activations well-scaled for ReLU layers |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7c1e2",
   "metadata": {},
   "source": [
    "Think of a deep network as a long pipe carrying information forward (activations) and backward (gradients).\n",
    "If initialization is poor:\n",
    "\n",
    "The signal fades out (too small weights)\n",
    "\n",
    "Or blows up (too large weights)\n",
    "\n",
    "Kaiming Normal keeps the flow balanced, especially for ReLU activations where half the neurons output 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37660102",
   "metadata": {},
   "source": [
    "| Concept                          | Meaning                               |\n",
    "| -------------------------------- | ------------------------------------- |\n",
    "| **Goal**                         | Prevent vanishing/exploding gradients |\n",
    "| **Best for**                     | ReLU and LeakyReLU                    |\n",
    "| **Weight distribution**          | Normal(0, 2 / fan_in)                 |\n",
    "| **Bias**                         | Safe to initialize to 0               |\n",
    "| **Alternative for Tanh/Sigmoid** | Xavier (Glorot) initialization        |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
